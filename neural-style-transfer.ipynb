{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Neural Style Transfer with Eager Execution","private_outputs":true,"provenance":[],"toc_visible":true,"version":"0.3.2"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7251,"sourceType":"datasetVersion","datasetId":2798},{"sourceId":9897,"sourceType":"datasetVersion","datasetId":6303}],"dockerImageVersionId":25160,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Style Transfer ","metadata":{"colab_type":"text","id":"jo5PziEC4hWs"}},{"cell_type":"code","source":"!wget --quiet https://upload.wikimedia.org/wikipedia/commons/d/d7/Green_Sea_Turtle_grazing_seagrass.jpg\n!wget --quiet https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg","metadata":{"colab":{},"colab_type":"code","id":"riWE_b8k3s6o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow.keras as kr\nimport tensorflow as tf\nimport numpy as np\nfrom IPython import display\nfrom PIL import Image\n\n\ntf.enable_eager_execution()\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))","metadata":{"colab":{},"colab_type":"code","id":"sc1OLbOWhPCO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CONTENT = 'Green_Sea_Turtle_grazing_seagrass.jpg'\nSTYLE = 'The_Great_Wave_off_Kanagawa.jpg'\n\nIMAGE_HEIGHT = 300\nIMAGE_WIDTH = 400","metadata":{"colab":{},"colab_type":"code","id":"IOiGrIV1iERH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize the input","metadata":{"colab_type":"text","id":"xE4Yt8nArTeR"}},{"cell_type":"code","source":"content = Image.open(CONTENT)\nstyle = Image.open(STYLE)\n\nplt.figure(figsize=(10, 10))\n\nplt.subplot(1, 2, 1)\nplt.imshow(content)\nplt.title('Content Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(style)\nplt.title('Style Image')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def img_parser(filename):\n    img_string = tf.io.read_file(filename)\n    img = tf.image.decode_jpeg(img_string, channels=3)\n    img = tf.cast(img, dtype=tf.float32)\n\n    # Resize the image\n    img = tf.image.resize_images(img, size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n    img = tf.expand_dims(img, axis=0)   # Add batch dimension\n    return img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image(filename):\n    img = img_parser(filename)\n    img = kr.applications.vgg19.preprocess_input(img)\n    return img","metadata":{"colab":{},"colab_type":"code","id":"hGwmTwJNmv2a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In order to view the outputs of our optimization, we are required to perform the inverse preprocessing step. Also,  we must clip to maintain our values from within the 0-255 range.   ","metadata":{"colab_type":"text","id":"xCgooqs6tAka"}},{"cell_type":"code","source":"def deprocess_img(processed_img):\n    x = processed_img.copy()\n    if len(x.shape) == 4:\n        x = np.squeeze(x, 0)\n    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n    if len(x.shape) != 3:\n        raise ValueError(\"Invalid input to deprocessing image\")\n  \n    # perform the inverse of the preprocessing step\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    x = x[:, :, ::-1]  # Convert back to RGB from BGR\n\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","metadata":{"colab":{},"colab_type":"code","id":"mjzlKRQRs_y2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_vgg = kr.applications.vgg19.VGG19(include_top=False, \n                                   weights=None, \n                                   input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n_vgg.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Content layer where will pull our feature maps\ncontent_layers = ['block5_conv2'] \n\n# Style layers\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']","metadata":{"colab":{},"colab_type":"code","id":"N4-8eUp_Kc-j","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(styles, contents):\n    \"\"\" Creates our model with access to intermediate layers. \n    Loads the VGG19 model and access the intermediate layers. \n    These layers will then be used to create a new model that will take input image\n    and return the outputs from these intermediate layers from the VGG model.\n    \n    Parameters:\n    -----------\n    styles: list\n        A list containing all style layers from VGG19 model\n    contents: list\n        A list containing all content layers from VGG19 model\n\n    Returns:\n    --------\n    VGG: kr.Model\n        A keras model that takes image inputs and outputs the style and \n        content intermediate layers. \n    \"\"\"\n    \n    vgg = kr.applications.vgg19.VGG19(include_top=False, \n                                      weights='imagenet', \n                                      input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n    vgg.trainable = False\n    \n    # Get output layers corresponding to style and content layers \n    style_outputs = [vgg.get_layer(layer_name).output for layer_name in styles]\n    content_outputs = [vgg.get_layer(layer_name).output for layer_name in contents]\n    model_outputs = style_outputs + content_outputs\n   \n    return kr.Model(vgg.input, model_outputs)","metadata":{"colab":{},"colab_type":"code","id":"nfec6MuMAbPx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_content_loss(content, generated):\n    return tf.reduce_mean(tf.square(content - generated))","metadata":{"colab":{},"colab_type":"code","id":"d2mf7JwRMkCd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_layer_style_loss(style, generated):\n    def gram_matrix(tensor):\n        channels = int(tensor.shape[-1])\n        a = tf.reshape(tensor, [-1, channels])\n        gram = tf.matmul(a, a, transpose_a=True)\n        return gram / tf.cast(tf.shape(a)[0], tf.float32)\n\n    gram_style = gram_matrix(style)\n    gram_generated = gram_matrix(generated)\n    return tf.reduce_mean(tf.square(gram_style - gram_generated))\n\n\ndef get_style_loss(style, generated):\n    loss = 0\n    coeffs = [0.2, 0.2, 0.2, 0.2, 0.2]\n    for s, g, coeff in zip(style, generated, coeffs):\n        loss += coeff * get_layer_style_loss(s, g)\n    \n    return loss","metadata":{"colab":{},"colab_type":"code","id":"N7MOqwKLLke8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(model, image, style_features, content_features, alpha=0.1, beta=0.002):\n    \"\"\"This function will compute the loss total loss.\n  \n    Parameters:\n    -----------\n    model: kr.Model \n        The model that will give us access to the intermediate layers\n    image: Tensor\n        Initial image. This is what we are updating with the optimization process. \n    style_features: Tensor\n        Precomputed style features from our Style image.\n    content_features: Tensor\n        Precomputed content features from our Content image.\n      \n    Returns:\n    loss: Tensor\n        Returns the total loss\n    \"\"\"\n\n    # Feed our init image through our model.\n    model_outputs = model(image)\n\n    content_generated = [content_layer[0] for content_layer in model_outputs[len(style_layers):]][0]\n    style_generated = [style_layer for style_layer in model_outputs[:len(style_layers)]]\n    \n    content_loss = alpha * get_content_loss(content_features, content_generated)\n    style_loss = beta * get_style_loss(style_features, style_generated)\n\n    # Get total loss\n    loss = style_loss + content_loss\n    return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_grads(cfg):\n    with tf.GradientTape() as tape: \n        loss = compute_loss(**cfg)\n    # Compute gradients with respect to input image\n    return tape.gradient(loss, cfg['image']), loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transfer_style(content_img, style_img, epochs=1000): \n    def generate_noisy_image(content_image, noise_ratio):\n        \"\"\"Generates a noisy image by adding random noise to the content image\"\"\"\n    \n        noise_image = tf.random.uniform([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3], minval=-20, maxval=20)\n        input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n        return input_image\n\n    # We don't want to train any layers of our model \n    model = get_model(style_layers, content_layers) \n    for layer in model.layers:\n        layer.trainable = False\n        \n    S = load_image(style_img)\n    C = load_image(content_img)\n\n    style_outputs = model(S)\n    content_outputs = model(C)\n\n    # Get the style and content feature representations (from our specified intermediate layers) \n    _content = [content_layer[0] for content_layer in content_outputs[len(style_layers):]][0]\n    _style = [style_layer[0] for style_layer in style_outputs[:len(style_layers)]]\n  \n    # Set initial image\n    G = generate_noisy_image(C, 0.6)\n    G = tf.contrib.eager.Variable(G, dtype=tf.float32)\n\n    best_loss, best_img = float('inf'), None\n  \n    # Create a nice config \n    cfg = {\n        'model': model,\n        'image': G,\n        'style_features': _style,\n        'content_features': _content\n    }\n    \n    # Create our optimizer\n    opt = tf.train.AdamOptimizer(learning_rate=2, beta1=0.99, epsilon=1e-1)\n    \n    # For displaying\n    display_interval = epochs/(2*5)\n  \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        grads, cost = compute_grads(cfg)\n        opt.apply_gradients([(grads, G)])\n        clipped = tf.clip_by_value(G, min_vals, max_vals)\n        G.assign(clipped)\n        \n        if cost < best_loss:\n            best_loss = cost\n            best_img = deprocess_img(G.numpy())\n\n        if i % display_interval== 0:\n            plot_img = G.numpy()\n            plot_img = deprocess_img(plot_img)\n            imgs.append(plot_img)\n            display.clear_output(wait=True)\n            display.display_png(Image.fromarray(plot_img))\n            print('Epoch: {}, LOSS: {:.4e}'.format(i, cost))\n        \n            \n    display.clear_output(wait=True)\n    plt.figure(figsize=(14,4))\n    for i,img in enumerate(imgs):\n        plt.subplot(2, 5, i+1)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n      \n    return best_img, best_loss ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best, best_loss = transfer_style(CONTENT, STYLE, epochs=200)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Outputs ","metadata":{"colab_type":"text","id":"LwiZfCW0AZwt"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\ncontent = Image.open(CONTENT) \nstyle = Image.open(STYLE)\n\nplt.subplot(1, 2, 1)\nplt.imshow(content)\nplt.title('Content Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(style)\nplt.title('Style Image')\n\nplt.figure(figsize=(10, 10))\n\nplt.imshow(best)\nplt.title('Output Image')\nplt.show()","metadata":{"colab":{},"colab_type":"code","id":"lqTQN1PjulV9","trusted":true},"outputs":[],"execution_count":null}]}